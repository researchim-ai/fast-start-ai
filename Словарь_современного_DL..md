# Словарь современного Deep Learning

Добро пожаловать в "Словарь современного Deep Learning" – исчерпывающий справочник по терминологии, методам и архитектурам, используемым в современном искусственном интеллекте.

Здесь вы найдёте подробные определения, объяснения и расширенные описания ключевых понятий.

## Оглавление

- **Общий DL** – базовые принципы и методы глубокого обучения.
- **LLM — большие языковые модели** – терминология и концепции для языковых моделей и обработки естественного языка.
- **Computer Vision** – термины, используемые в области компьютерного зрения.
- **Reinforcement Learning** – основные понятия и алгоритмы обучения с подкреплением.
- **Additional Topics and Methods** – дополнительные техники и подходы в Deep Learning.
- **Audio and Speech Processing** – термины, связанные с обработкой аудио и речью.
- **Time Series Analysis and Forecasting** – основы анализа временных рядов и прогнозирования.
- **Variational Methods and Normalizing Flows** – ключевые концепции вариационных методов и нормализующих потоков.
- **Recommender Systems** – терминология систем рекомендаций.
- **Robotics and Control** – понятия, используемые в робототехнике и системах управления.
- **Interpretability and Explainability** – техники интерпретации и объяснения решений моделей.
- **Edge AI and Deployment** – методы оптимизации и развертывания моделей на периферийных устройствах.

---

## Общий DL

**Deep Learning – Глубокое обучение**  
Область машинного обучения, использующая нейронные сети с множеством слоёв для решения сложных задач. Эта методика позволяет моделям автоматически извлекать высокоуровневые абстракции из данных, что делает её эффективной для распознавания образов, обработки естественного языка и других сложных задач.

**Neural Network – Нейронная сеть**  
Модель, состоящая из взаимосвязанных нейронов (узлов), имитирующих работу человеческого мозга для обработки информации. Нейронные сети способны моделировать сложные зависимости между входными и выходными данными, что делает их ключевым элементом в решении задач классификации, регрессии и генерации данных.

**Feedforward Neural Network – Прямая нейронная сеть**  
Базовый тип сети, в которой данные проходят от входного слоя к выходному без обратных связей. Такие сети используются для задач, где важна прямая связь между входом и выходом, без учета временных зависимостей.

**Perceptron – Перцептрон**  
Элементарная модель нейрона, принимающая входные данные, обрабатывающая их и выдающая результат. Перцептрон демонстрирует принцип линейной классификации и служит фундаментом для построения более сложных архитектур.

**Fully Connected Layer – Полносвязный слой**  
Слой, в котором каждый нейрон соединён с каждым нейроном предыдущего слоя. Он позволяет моделям обучаться сложным нелинейным зависимостям за счёт оптимизации большого числа весовых коэффициентов.

**Activation Function – Функция активации**  
Функция, применяемая к сумме взвешенных входов нейрона для введения нелинейности (например, ReLU, sigmoid, tanh). Нелинейность позволяет сети моделировать сложные зависимости, выходящие за рамки линейных отношений.

**Softmax – Софтмакс**  
Функция, преобразующая вектор значений в вероятностное распределение, часто используемая для многоклассовой классификации. Она нормализует выходы так, чтобы их сумма равнялась единице, что позволяет интерпретировать их как вероятности.

**Loss Function – Функция потерь**  
Мера ошибки, определяющая разницу между предсказанными и истинными значениями, которую модель стремится минимизировать. Функция потерь направляет процесс оптимизации, позволяя корректировать веса сети на основе величины ошибки.

**Gradient Descent – Метод градиентного спуска**  
Алгоритм оптимизации для корректировки весов модели в направлении наискорейшего уменьшения функции потерь. Метод итеративно обновляет параметры, используя вычисление производных, чтобы найти минимум функции потерь.

**Momentum – Моментум**  
Метод ускорения сходимости оптимизации за счёт учёта предыдущих обновлений весов для сглаживания траектории. Это помогает преодолеть локальные минимумы и ускоряет обучение глубоких моделей.

**Adam – Адам**  
Популярный алгоритм оптимизации, объединяющий адаптивное вычисление моментов и метод моментума для ускорения сходимости. Он автоматически настраивает скорость обучения для каждого параметра, что делает его универсальным и эффективным.

**Backpropagation – Обратное распространение ошибки**  
Метод расчёта градиентов, используемый для обновления весов нейронной сети. Он распространяет ошибку от выходного слоя к входному, позволяя корректировать веса на основе вклада каждого нейрона в итоговую ошибку.

**Optimizer – Оптимизатор**  
Алгоритм (например, SGD, Adam, RMSprop), используемый для обновления параметров модели в процессе обучения. Выбор оптимизатора влияет на скорость сходимости и качество итоговой модели.

**Regularization – Регуляризация**  
Набор техник для уменьшения переобучения модели, таких как L1, L2, Dropout и Early Stopping. Регуляризация добавляет ограничения к функции потерь или структуре сети, улучшая обобщающие способности модели.

**Dropout – Отбрасывание**  
Метод регуляризации, при котором случайным образом исключаются некоторые нейроны во время обучения. Это помогает предотвратить избыточное запоминание обучающих данных и улучшает способность модели обобщать.

**Batch Normalization – Нормализация пакета**  
Техника, нормализующая входы каждого слоя по мини-пакетам данных для стабилизации и ускорения обучения. Нормализация снижает влияние изменений распределения входных данных, что способствует быстрой и стабильной сходимости.

**Learning Rate – Скорость обучения**  
Гиперпараметр, определяющий величину шага при обновлении весов модели. Правильный выбор скорости обучения критически важен: слишком высокий может привести к нестабильности, а слишком низкий — к медленной сходимости.

**Learning Rate Scheduler – Планировщик скорости обучения**  
Механизм динамического изменения скорости обучения в процессе тренировки, позволяющий модели тонко настраиваться на поздних этапах оптимизации.

**Epoch – Эпоха**  
Один полный проход по всему набору тренировочных данных, после которого происходит обновление параметров модели на основе полной выборки.

**Mini-Batch – Мини-пакет**  
Небольшая часть обучающих данных, используемая для одного шага обновления весов. Разбиение данных на мини-пакеты уменьшает вычислительную нагрузку и способствует стабильности обучения.

**Weight Initialization – Инициализация весов**  
Процесс задания начальных значений параметров нейронной сети, что влияет на скорость и качество обучения. Хорошая инициализация помогает избежать проблем с затухающими или взрывающимися градиентами.

**Transfer Learning – Трансферное обучение**  
Метод адаптации модели, предварительно обученной на одном наборе данных, для решения смежной задачи. Это позволяет использовать уже извлечённые признаки и значительно сокращает время обучения.

**Model Pruning – Обрезка модели**  
Процесс удаления избыточных параметров для уменьшения размера модели и повышения её вычислительной эффективности. Обрезка снижает сложность модели без значительной потери точности.

**Quantization – Квантование**  
Снижение точности представления параметров модели с целью ускорения вычислений и экономии памяти, что особенно важно для развертывания моделей на мобильных устройствах.

**Ensemble Methods – Ансамблевые методы**  
Подход, при котором несколько моделей объединяются для получения более точных и устойчивых предсказаний. Ансамблирование позволяет компенсировать ошибки отдельных моделей и улучшать обобщение.

**Cross-Validation – Кросс-валидация**  
Метод оценки обобщающих способностей модели путём разбиения данных на несколько обучающих и тестовых подмножеств, что помогает выявить переобучение.

**Residual Network (ResNet) – Остаточная сеть**  
Архитектура с пропускными связями, позволяющими строить очень глубокие сети без проблем затухающего градиента. Пропуски позволяют передавать информацию напрямую через несколько слоёв.

**Graph Neural Network (GNN) – Графовая нейронная сеть**  
Модель для обработки структурированных данных в виде графов, где связи между объектами играют ключевую роль. GNN используются в задачах социальных сетей, молекулярной химии и других областях.

**Capsule Network – Капсульная сеть**  
Архитектура, сохраняющая пространственные иерархии между объектами, что полезно для задач распознавания изображений. Капсульные сети передают информацию о позиции и ориентации объектов, повышая устойчивость к трансформациям.

**DropConnect – Отбрасывание связей**  
Метод регуляризации, при котором случайным образом исключаются отдельные веса вместо целых нейронов, что улучшает обобщающую способность модели.

**Sparse Coding – Разреженное кодирование**  
Техника представления данных с использованием ограниченного числа активных элементов, способствующая выявлению наиболее значимых признаков.

**Overfitting – Переобучение**  
Ситуация, когда модель слишком хорошо запоминает обучающие данные, теряя способность обобщать на новые примеры. Способы борьбы включают регуляризацию и аугментацию данных.

**Underfitting – Недообучение**  
Состояние, при котором модель не способна уловить основные закономерности в данных, что приводит к низкой точности как на обучающем, так и на тестовом наборах.

**Early Stopping – Раннее прекращение обучения**  
Метод остановки обучения, когда качество модели на валидационном наборе перестаёт улучшаться. Это помогает предотвратить переобучение.

**Gradient Clipping – Ограничение градиента**  
Техника, предотвращающая взрыв градиентов при обучении глубоких сетей, ограничивая максимальное значение градиента.

**Data Normalization – Нормализация данных**  
Процесс приведения данных к единому масштабу, что улучшает сходимость модели и повышает стабильность обучения.

**Logits – Логиты**  
Сырые выходы последнего линейного слоя нейронной сети, которые часто подаются на функцию softmax для получения вероятностного распределения.

**Log Probability – Логарифмическая вероятность**  
Логарифм вычисленной вероятности, используемый для повышения числовой стабильности при расчётах функции потерь.

**Stochastic Gradient Descent (SGD) – Стохастический градиентный спуск**  
Вариант метода градиентного спуска, при котором обновление параметров производится на основе случайного мини-пакета данных.

**Learning Curve – График обучения**  
График, отображающий изменение производительности модели (например, точность или функцию потерь) в зависимости от количества эпох или итераций.

**Overparameterization – Переизбыточность параметров**  
Ситуация, когда модель имеет значительно больше параметров, чем необходимо для решения задачи, что может способствовать лучшей оптимизации, но требует регуляризации.

**Dimensionality Reduction – Снижение размерности**  
Методы уменьшения количества признаков в данных (например, PCA), используемые для визуализации или повышения эффективности обучения.

**Weight Decay – Затухание весов**  
Техника регуляризации, которая штрафует большие значения весов, способствуя улучшению обобщающих способностей модели.

**L1 Regularization – L1 регуляризация**  
Метод регуляризации, основанный на абсолютной величине весов, который способствует разреженности модели.

**L2 Regularization – L2 регуляризация**  
Метод регуляризации, основанный на квадрате весов, помогающий предотвращать переобучение за счёт снижения больших значений весов.

**DropBlock – Отбрасывание блоков**  
Структурированный метод регуляризации, который случайным образом обнуляет связные области в карте признаков для улучшения обобщения.

**Activation Maximization – Максимизация активации**  
Метод визуализации, оптимизирующий вход для максимизации активации определённых нейронов, что помогает понять, какие признаки обнаруживает сеть.

**Batch Size – Размер мини-пакета**  
Количество примеров, обрабатываемых перед обновлением параметров модели. От размера мини-пакета зависит скорость и стабильность обучения.

**Loss Surface – Поверхность потерь**  
Многомерное пространство функции потерь, через которое проходит оптимизатор, отражающее сложность задачи оптимизации.

**Hyperparameter Tuning – Настройка гиперпараметров**  
Процесс поиска оптимальных значений гиперпараметров (например, скорость обучения, размер мини-пакета) для повышения производительности модели.

---

## LLM — большие языковые модели

**LLM (Large Language Model) – Большая языковая модель**  
Комплексная нейросеть, обученная на огромном количестве текстовых данных для генерации, анализа и понимания естественного языка. Такие модели способны улавливать тонкости языка, контекст и смысл, что позволяет им выполнять широкий спектр задач – от перевода до создания творческих текстов.

**Language Modeling – Языковое моделирование**  
Задача предсказания следующего слова или символа в последовательности, лежащая в основе обучения языковых моделей. Этот процесс помогает модели понять структуру языка и взаимосвязи между словами.

**Pretraining – Предобучение**  
Этап обучения модели на большом корпусе текстов без специализированной разметки, позволяющий освоить базовые языковые закономерности до тонкой настройки на конкретную задачу.

**Masked Language Modeling – Замаскированное языковое моделирование**  
Метод, при котором часть токенов заменяется специальными масками, а модель обучается восстанавливать пропущенные слова, используя контекст окружающих токенов.

**Causal Language Modeling – Причинное языковое моделирование**  
Обучение модели предсказывать следующий токен на основе уже сгенерированного текста, что характерно для авторегрессионных моделей.

**Fine-tuning – Тонкая настройка**  
Доработка предобученной модели на специализированном наборе данных для повышения точности решения конкретной задачи, будь то перевод, суммирование или классификация.

**Parameter Efficient Fine-Tuning – Эффективное дообучение**  
Методы адаптации крупных моделей с минимальными изменениями параметров (например, LoRA), позволяющие экономить вычислительные ресурсы при сохранении высокой точности.

**Token – Токен**  
Минимальная единица текста (слово, часть слова или символ), с которой работает модель. Токенизация помогает разбивать текст на понятные единицы для дальнейшей обработки.

**Prompt – Промт/Запрос/Подсказка**  
Входной текст, подаваемый модели для генерации ответа, от которого зависит качество и релевантность результата.

**Context Window – Контекстное окно**  
Ограничение на количество токенов, которое модель может одновременно учитывать, влияющее на «память» и способность учитывать длинные зависимости в тексте.

**In-context Learning – Обучение в контексте**  
Способность модели адаптироваться к задаче на основе примеров, приведённых непосредственно в запросе, без дополнительного обучения.

**Zero-shot Learning – Обучение без примеров**  
Возможность модели выполнять задачу на основе только описания без предоставления примеров, демонстрируя высокий уровень обобщения.

**Few-shot Learning – Обучение с несколькими примерами**  
Метод, при котором в запросе предоставляется несколько примеров для улучшения понимания задачи моделью и повышения точности вывода.

**Chain-of-thought Reasoning – Цепочка рассуждений**  
Техника пошагового описания процесса решения задачи, позволяющая отследить логику, по которой модель приходит к ответу, что полезно для диагностики.

**Decoding – Декодирование**  
Процесс генерации текста из скрытых представлений модели; выбор алгоритма (например, жадный поиск или beam search) влияет на качество и разнообразие сгенерированного текста.

**Temperature – Температура**  
Гиперпараметр, регулирующий степень случайности в процессе генерации текста: низкое значение даёт более детерминированный вывод, а высокое – повышает креативность.

**Beam Search – Поиск по лучу**  
Алгоритм, одновременно рассматривающий несколько вариантов продолжения текста для выбора наиболее вероятного, что улучшает качество сгенерированного результата.

**Knowledge Distillation – Дистилляция знаний**  
Процесс переноса знаний от большой модели к меньшей, позволяющий сократить вычислительные затраты без значительной потери точности.

**Multitask Learning – Мультитаскинг**  
Обучение модели выполнению нескольких задач одновременно, что способствует улучшению обобщающих способностей за счёт использования взаимосвязанных задач.

**Model Scaling – Масштабирование модели**  
Увеличение числа параметров и вычислительной мощности модели для повышения её производительности, что часто приводит к улучшению качества предсказаний.

**Attention Mask – Маска внимания**  
Механизм, определяющий, какие токены учитывать при вычислении внимания, что помогает избежать влияния паддинга и нерелевантной информации.

**Positional Encoding – Позиционное кодирование**  
Метод добавления информации о положении токенов в последовательности для сохранения порядка слов, что критично для понимания контекста.

**Pretraining Objectives – Цели предобучения**  
Набор задач (например, Next Sentence Prediction, Permuted Language Modeling) для обучения базовым языковым закономерностям до тонкой настройки модели.

**Scaling Laws – Законы масштабирования**  
Эмпирические зависимости, описывающие, как увеличение размера модели и объёма данных влияет на её производительность и точность.

**Multilingual Modeling – Мультиязычное моделирование**  
Подход, позволяющий обучать модели, способные работать с несколькими языками, что расширяет их применение на глобальном уровне.

**Next Sentence Prediction – Предсказание следующего предложения**  
Задача, в которой модель определяет, является ли следующее предложение логически связанным с предыдущим, что помогает уловить структуру текста.

**Text Summarization – Резюмирование текста**  
Процесс создания краткого изложения основного содержания документа, что полезно для автоматического создания аннотаций и обзоров.

**Prompt Tuning – Тонкая настройка запросов**  
Метод оптимизации формулировки входных запросов для улучшения качества и релевантности ответов модели.

**Encoder-Decoder Model – Модель энкодер-декодер**  
Архитектура, состоящая из двух компонентов: энкодера для преобразования входного текста в скрытое представление и декодера для генерации ответа.

**Autoregressive Model – Авторегрессионная модель**  
Модель, генерирующая текст последовательно, предсказывая следующий токен на основе уже сгенерированных, что обеспечивает связность и непрерывность вывода.

**Transformer Architecture – Архитектура трансформера**  
Нейронная сеть, основанная на механизме внимания, которая революционизировала обработку естественного языка благодаря своей способности моделировать длинные зависимости.

**Self-Attention – Самовнимание**  
Механизм, позволяющий модели оценивать взаимосвязи между всеми элементами входной последовательности для формирования более богатых представлений.

**Multi-Head Attention – Многоголовое внимание**  
Расширение механизма самовнимания, позволяющее параллельно использовать несколько «голов», каждая из которых изучает различные аспекты входных данных.

**Byte Pair Encoding (BPE) – Кодирование пар байтов**  
Метод токенизации, который разбивает текст на подсловные единицы на основе частотного анализа, улучшая обработку редких слов.

**Subword Tokenization – Токенизация на подслова**  
Метод деления слов на более мелкие единицы для борьбы с проблемой отсутствия словаря для редких или новых слов.

**Layer Normalization – Нормализация слоев**  
Метод нормализации, применяемый к входным данным внутри слоя, который помогает стабилизировать и ускорить обучение глубоких сетей.

**Prompt Chaining – Цепочка промтов**  
Метод последовательного объединения нескольких запросов для генерации более точных и контекстуально согласованных ответов.

**Tokenization Strategy – Стратегия токенизации**  
Подходы к разбиению текста на токены, включая методы типа Unigram, SentencePiece и другие, что влияет на эффективность обработки текста.

**Decoding Strategies – Стратегии декодирования**  
Методы генерации текста, такие как top-k sampling, nucleus sampling и другие, определяющие разнообразие и креативность выходного текста.

**Self-Consistency – Самоконстистентность**  
Подход, при котором генерируется несколько вариантов ответа, после чего выбирается наиболее согласованный, что повышает точность вывода.

**Alignment – Выравнивание**  
Процесс настройки модели для соответствия ожиданиям и этическим нормам, что важно для обеспечения безопасного и полезного использования.

**Pretraining Corpus – Корпус для предобучения**  
Большой набор текстов, на котором проходит начальное обучение модели, влияющий на её знания и способность к генерации текста.

**Zero-shot Prompting – Запросы без примеров**  
Метод, позволяющий модели выполнять задачи, основываясь только на текстовом описании, без предоставления примеров, демонстрируя высокую степень обобщения.

---

## Computer Vision

**Image Classification – Классификация изображений**  
Задача определения категории, к которой принадлежит изображение, посредством анализа визуальных признаков с использованием специализированных моделей.

**Object Detection – Обнаружение объектов**  
Процесс локализации и классификации объектов на изображении с помощью рамок или масок, позволяющий определять как класс, так и положение объектов.

**Semantic Segmentation – Семантическая сегментация**  
Разбиение изображения на области, соответствующие различным классам, где каждому пикселю присваивается определённый класс без выделения отдельных экземпляров.

**Instance Segmentation – Сегментация объектов**  
Метод, позволяющий выделять отдельные объекты одного класса и определять их индивидуальные границы в рамках одного изображения.

**Convolutional Neural Network (CNN) – Сверточная нейронная сеть**  
Архитектура, оптимизированная для обработки изображений с помощью свёрток, что позволяет эффективно извлекать пространственные признаки.

**Convolution – Свёртка**  
Операция, применяемая для извлечения локальных признаков из изображения посредством применения фильтров, выявляющих границы, текстуры и формы.

**Pooling – Подвыборка**  
Операция уменьшения пространственных размеров представлений (например, max pooling или average pooling) для выделения наиболее устойчивых признаков и снижения вычислительной нагрузки.

**Feature Map – Карта признаков**  
Результат применения свёрточного фильтра к изображению, содержащий обнаруженные визуальные признаки, которые используются для последующего анализа.

**Image Preprocessing – Предобработка изображений**  
Методы подготовки изображений к анализу, включая нормализацию, изменение размера и аугментацию, что улучшает качество обучения модели.

**Image Augmentation – Аугментация изображений**  
Методы искусственного увеличения объёма данных посредством применения трансформаций (поворот, масштабирование, отражение), что помогает избежать переобучения.

**Edge Detection – Выделение контуров**  
Алгоритмы для обнаружения границ объектов, способствующие выделению структурных особенностей изображения, важных для анализа формы.

**Optical Flow – Оптический поток**  
Метод оценки движения объектов в последовательности изображений или видеопотоке, позволяющий отслеживать динамику и скорость перемещений.

**Pose Estimation – Определение позы**  
Задача определения положения и ориентации человека или объекта на изображении, используемая в системах распознавания действий и анимации.

**Generative Adversarial Networks (GAN) в CV – Генеративно-состязательные сети**  
Применение GAN для генерации реалистичных изображений, улучшения качества или стилизации, где генератор и дискриминатор обучаются в противоборстве.

**Visual Transformers – Визуальные трансформеры**  
Адаптация архитектуры трансформеров для обработки изображений, где механизм внимания помогает выделять ключевые признаки на разных масштабах.

**Region Proposal Network (RPN) – Сеть предложений регионов**  
Компонент для генерации кандидатов областей на изображении, потенциально содержащих объекты, что ускоряет процесс обнаружения.

**Feature Pyramid Network (FPN) – Сеть пирамиды признаков**  
Архитектура, объединяющая признаки на различных масштабах для улучшения обнаружения объектов разного размера.

**Style Transfer – Перенос стиля**  
Метод, позволяющий перенести художественный стиль одного изображения на контент другого, создавая новые визуальные эффекты.

**Super-Resolution – Сверхразрешение**  
Техника повышения разрешения изображения с восстановлением утраченных деталей, используемая для улучшения качества низкоразрешённых изображений.

**Image Denoising – Удаление шума**  
Методы очистки изображений от случайных искажений и артефактов, что улучшает качество входных данных для дальнейшей обработки.

**Image Inpainting – Восстановление изображений**  
Процесс заполнения пропущенных или повреждённых частей изображения на основе окружающих пикселей, используемый для реставрации и редактирования.

**3D Reconstruction – 3D реконструкция**  
Методы создания трёхмерной модели объекта или сцены на основе одного или нескольких изображений, что важно для компьютерной графики и робототехники.

**Object Tracking – Отслеживание объектов**  
Технологии, позволяющие непрерывно определять положение и перемещения объектов в видеопотоке, что важно для видеонаблюдения и автономного вождения.

**Optical Character Recognition (OCR) – Оптическое распознавание символов**  
Методы преобразования изображений с текстом в редактируемый текстовый формат, широко используемые для автоматизации ввода данных.

**Receptive Field – Поле восприятия**  
Область входного изображения, на которую влияет конкретный нейрон в свёрточном слое, определяющая масштабы обнаруживаемых признаков.

**Histogram of Oriented Gradients (HOG) – Гистограмма ориентированных градиентов**  
Метод дескрипции признаков, используемый для детектирования объектов на изображениях на основе распределения направлений градиентов.

**Scale-Invariant Feature Transform (SIFT) – SIFT (масштабно-инвариантное преобразование признаков)**  
Алгоритм для обнаружения и описания локальных признаков, устойчивых к масштабным и поворотным преобразованиям.

**Speeded Up Robust Features (SURF) – SURF (ускоренные устойчивые признаки)**  
Быстрый алгоритм для обнаружения и описания ключевых точек на изображениях, служащий альтернативой SIFT.

**Panoptic Segmentation – Паноптическая сегментация**  
Задача, объединяющая семантическую и инстанс-сегментацию для получения полного понимания сцены, где каждому пикселю присваивается как класс, так и идентификатор объекта.

**Image Captioning – Описание изображения**  
Задача генерации текстового описания для заданного изображения, объединяющая компьютерное зрение и обработку естественного языка.

**Color Jitter – Изменение цвета**  
Техника аугментации, изменяющая яркость, контраст, насыщенность и оттенок изображения для повышения разнообразия обучающих данных.

**Image Normalization – Нормализация изображений**  
Процесс масштабирования значений пикселей к единому диапазону, что улучшает стабильность и скорость обучения модели.

**Object Proposal – Предложение объектов**  
Методы генерации кандидатных регионов, которые потенциально могут содержать объекты, используемые в алгоритмах обнаружения.

**Semantic Embedding – Семантическое встраивание**  
Преобразование изображений или их частей в векторное пространство, отражающее их семантическое содержание и сходство.

**Feature Extraction Backbone – Бэкбон для извлечения признаков**  
Сверточная сеть, используемая для извлечения признаков из изображений, служащая основой для задач обнаружения и сегментации.

**Non-Maximum Suppression (NMS) – Подавление немаксимумов**  
Алгоритм, используемый для устранения избыточных предложений объектов путём выбора только наилучших вариантов.

**Anchor Boxes – Якорные рамки**  
Предопределённые рамки различных размеров и соотношений сторон, применяемые для локализации объектов в алгоритмах обнаружения.

---

## Reinforcement Learning

**Reinforcement Learning – Обучение с подкреплением**  
Область, в которой агент обучается на основе наград и штрафов, получаемых от взаимодействия с окружающей средой. Такой подход позволяет моделям самостоятельно изучать оптимальные стратегии на основе опыта.

**Q-Learning – Q-обучение**  
Метод, при котором агент изучает оптимальную политику, оценивая Q-функцию — ожидаемую награду для пары состояние-действие, что помогает выбирать наиболее выгодные действия.

**Deep Q-Network (DQN) – Глубокая Q-сеть**  
Комбинация Q-обучения и нейронных сетей, позволяющая работать с высокоразмерными пространствами состояний и применять обучение с подкреплением в сложных средах.

**Policy Gradient – Градиент политики**  
Метод оптимизации, при котором параметры политики обновляются напрямую для максимизации ожидаемой награды, что позволяет агенту обучаться стратегии без явной оценки всех вариантов.

**Actor-Critic – Актор-критик**  
Подход, где «актор» принимает решения, а «критик» оценивает их, помогая корректировать стратегию. Такой метод сочетает преимущества оценки ценности и прямой оптимизации политики.

**Proximal Policy Optimization (PPO) – Проксимальное оптимальное обучение политики**  
Алгоритм, обеспечивающий стабильное обновление политики посредством ограничения изменений, что помогает избежать резких скачков в обучении.

**Monte Carlo Tree Search (MCTS) – Монте-Карло поиск по дереву**  
Метод планирования, использующий симуляцию множества вариантов развития событий для выбора оптимальных действий, широко применяемый в стратегических играх.

**Exploration vs. Exploitation – Исследование vs. использование**  
Баланс между поиском новых стратегий (исследование) и применением уже известных для максимизации награды, критически важный для успешного обучения.

**SARSA – SARSA алгоритм**  
Метод обучения с подкреплением, где обновление производится на основе текущего действия и следующего шага, отражая реальное поведение агента.

**Double DQN – Двойная глубокая Q-сеть**  
Улучшенная версия DQN, снижающая переоценку Q-значений за счёт двойного оценивания, что обеспечивает более стабильное обучение.

**Prioritized Experience Replay – Приоритетный повтор опыта**  
Метод, позволяющий чаще использовать для обучения наиболее значимые переходы, ускоряя процесс обучения за счёт фокусировки на ошибках.

**Reward Shaping – Формирование награды**  
Методика модификации функции награды для направления агента к желаемому поведению, что помогает ускорить обучение.

**Discount Factor – Коэффициент дисконтирования**  
Параметр, определяющий важность будущих наград по сравнению с текущими, влияющий на стратегию агента в долгосрочной перспективе.

**Exploration Strategies – Стратегии исследования**  
Набор методов (например, ε-greedy, Boltzmann exploration) для обеспечения баланса между исследованием новых действий и использованием известных выгодных стратегий.

**Eligibility Traces – Следы пригодности**  
Механизм, позволяющий учитывать влияние предыдущих состояний и действий при обновлении политики, что ускоряет обучение в задачах с разреженной наградой.

**Policy Regularization – Регуляризация политики**  
Методы ограничения изменений в политике для предотвращения чрезмерных колебаний в стратегии агента, способствующие стабильному обучению.

**Temporal Difference (TD) Learning – Обучение с временной разницей**  
Метод, который обновляет оценки на основе разницы между предсказанными и фактическими наградами, без ожидания полного завершения эпизода.

**Bellman Equation – Уравнение Беллмана**  
Рекурсивное уравнение, описывающее соотношение между ценностью состояния и ценностями его последующих состояний.

**Deterministic Policy Gradient (DPG) – Градиент детерминированной политики**  
Подход, при котором политика является детерминированной, а градиенты вычисляются напрямую по параметрам политики.

**Deep Deterministic Policy Gradient (DDPG) – Глубокий детерминированный градиент политики**  
Алгоритм, расширяющий DPG с использованием глубоких нейронных сетей, предназначенный для задач с непрерывным пространством действий.

**Twin Delayed Deep Deterministic Policy Gradient (TD3) – Двойной замедленный глубокий детерминированный градиент политики**  
Улучшенная версия DDPG, снижающая переоценку ценностей с помощью двойных критиков и задержанных обновлений.

**Soft Actor-Critic (SAC) – Мягкий актор-критик**  
Оф-политический алгоритм актор-критика, который включает энтропийный член в функцию цели для поощрения исследования.

**Hierarchical Reinforcement Learning – Иерархическое обучение с подкреплением**  
Подход, разлагающий сложную задачу на иерархию субзадач, что облегчает обучение сложных стратегий.

**Multi-Step Returns – Многошаговые вознаграждения**  
Метод, использующий накопленные награды за несколько шагов для обновления функции ценности, что помогает сбалансировать смещение и дисперсию.

**Intrinsic Motivation – Внутренняя мотивация**  
Дополнительный сигнал награды, стимулирующий агента к исследованию даже при отсутствии внешних стимулов.

**Options Framework – Фреймворк опций**  
Метод в иерархическом обучении с подкреплением, где действия представляют собой длительные, высокоуровневые стратегии.

**Off-Policy Learning – Офф-политическое обучение**  
Обучение с использованием данных, собранных другой, не текущей политикой агента, что позволяет повторно использовать прошлый опыт.

**On-Policy Learning – Он-политическое обучение**  
Обучение, при котором данные собираются непосредственно текущей политикой агента, что обеспечивает точное соответствие оценок.

**Batch RL – Пакетное обучение с подкреплением**  
Обучение на заранее собранном наборе данных без активного взаимодействия с средой, что позволяет повторно использовать исторические данные.

**Exploration Bonus – Бонус за исследование**  
Дополнительное вознаграждение, стимулирующее агента к исследованию новых или редко посещаемых состояний.

**Model-Based RL – Модельное обучение с подкреплением**  
Подход, при котором агент использует модель окружающей среды для планирования и принятия решений, что может ускорить обучение.

**Value Function – Функция ценности**  
Функция, оценивающая ожидаемую суммарную награду для заданного состояния или пары состояние-действие, критически важная для выбора стратегии.

**Policy Iteration – Итерация политики**  
Метод, чередующий оценку функции ценности и улучшение политики до достижения оптимума.

**Value Iteration – Итерация ценности**  
Метод обновления функции ценности до сходимости, после чего извлекается оптимальная политика для принятия решений.

**Distributional RL – Распределительное обучение с подкреплением**  
Подход, моделирующий полное распределение наград, а не только их математическое ожидание, для более точного представления неопределённости.

**Off-Policy Correction – Коррекция офф-политических данных**  
Методы для устранения смещения, возникающего при использовании данных, собранных другой политикой, в процессе обучения.

---

## Audio and Speech Processing

**Mel-Frequency Cepstral Coefficients (MFCC) – Мел-частотные кепстральные коэффициенты**  
Представление аудио сигналов, часто используемое для распознавания речи благодаря адаптации к особенностям человеческого слуха.

**Spectrogram – Спектрограмма**  
Визуальное представление распределения частот аудиосигнала во времени, которое помогает анализировать его структуру.

**Voice Activity Detection (VAD) – Определение активности голоса**  
Техника, позволяющая определить наличие речи в аудиосигнале, отделяя её от фонового шума.

**End-to-End Speech Recognition – Конечное распознавание речи**  
Подход, при котором модель напрямую преобразует аудио сигнал в текст, минуя этапы традиционного разделения на акустическую и языковую модели.

**WaveNet – ВейвНет**  
Генеративная модель, основанная на сверточных нейронных сетях с дилатацией, используемая для синтеза высококачественного аудио.

**Tacotron – Такотрон**  
Модель для синтеза речи, преобразующая текст в мел-спектрограмму с последующим преобразованием в аудиосигнал.

**Acoustic Model – Акустическая модель**  
Модель, которая преобразует аудио сигналы в набор признаков для последующего распознавания речи.

**Language Model (для ASR) – Языковая модель**  
Модель, предсказывающая последовательность слов, используемая для корректировки результатов распознавания речи.

**Feature Extraction in Audio – Извлечение признаков в аудио**  
Процесс преобразования сырого аудиосигнала в информативное представление (например, MFCC, спектрограмма) для обучения модели.

**Data Augmentation for Audio – Аугментация аудио данных**  
Методы изменения аудиосигналов (например, добавление шума, изменение скорости воспроизведения) для увеличения разнообразия обучающего набора.

**Spectral Centroid – Центроид спектра**  
Характеристика, определяющая «центр массы» спектра, которая отражает яркость или резкость звука.

**Chroma Feature – Хрома-признаки**  
Представление аудиосигнала, основанное на распределении энергии по 12-х музыкальным полутональным составляющим.

**Formants – Форманты**  
Ключевые резонансные частоты, определяющие тембр и качество звука речи.

**Prosody – Просодия**  
Интонационные и ритмические характеристики речи, влияющие на её эмоциональное и семантическое восприятие.

**End-to-End TTS – Конечное преобразование текста в речь**  
Модель, преобразующая текст напрямую в аудиосигнал без явного разделения на акустическую и языковую модели.

---

## Time Series Analysis and Forecasting

**ARIMA – ARIMA**  
Модель авторегрессии интегрированного скользящего среднего, широко используемая для прогнозирования временных рядов.

**Seasonality – Сезонность**  
Повторяющийся паттерн в данных, обусловленный периодическими изменениями, например, по сезонам или дням недели.

**Trend Analysis – Анализ тренда**  
Процесс выявления долгосрочного направления изменений временного ряда, что помогает предсказывать будущее поведение данных.

**Stationarity – Стационарность**  
Свойство временного ряда, когда его статистические характеристики (среднее, дисперсия) остаются постоянными во времени.

**Exponential Smoothing – Экспоненциальное сглаживание**  
Метод сглаживания временных рядов для выделения трендов и сезонных эффектов, применяемый для прогнозирования.

**Prophet – Prophet**  
Инструмент прогнозирования временных рядов, разработанный Facebook, который учитывает тренды, сезонность и праздники.

**Anomaly Detection in Time Series – Выявление аномалий во временных рядах**  
Методы обнаружения необычных событий или изменений, отклоняющихся от нормального поведения данных.

**Change Point Detection – Определение точек изменения**  
Алгоритмы, которые определяют моменты, когда статистические свойства временного ряда существенно меняются.

**Recurrent Neural Networks for Time Series – Рекуррентные нейронные сети для временных рядов**  
Применение RNN для моделирования временных зависимостей и прогнозирования будущих значений ряда.

**Temporal Convolutional Network (TCN) – Временная сверточная сеть**  
Архитектура, использующая сверточные слои для обработки последовательностей, обеспечивающая долговременные зависимости в данных.

**Autocorrelation – Автокорреляция**  
Мера зависимости текущего значения временного ряда от его прошлых значений, используемая для выявления повторяющихся паттернов.

**Cross-Correlation – Кросс-корреляция**  
Мера взаимосвязи между двумя временными рядами, позволяющая выявить их совместные зависимости.

**Lag – Лаг**  
Временное смещение, используемое для анализа зависимости между текущими и прошлыми значениями временного ряда.

**Rolling Statistics – Скользящая статистика**  
Вычисление статистических показателей (среднее, дисперсия) на скользящем окне для выявления локальных особенностей временного ряда.

**Exogenous Variables – Экзогенные переменные**  
Внешние факторы, влияющие на временной ряд (например, погода, праздники), которые могут быть включены в модель прогнозирования.

---

## Variational Methods and Normalizing Flows

**Variational Autoencoder (VAE) – Вариационный автоэнкодер**  
Генеративная модель, использующая вариационный вывод для аппроксимации распределения данных через латентное пространство.

**Reparameterization Trick – Трюк репараметризации**  
Метод, позволяющий проводить обратное распространение через стохастические узлы, что критически важно для обучения VAE.

**Evidence Lower Bound (ELBO) – Нижняя оценка доказательства**  
Целевая функция в вариационных методах, которую максимизируют, чтобы приблизить апостериорное распределение к истинному.

**Normalizing Flow – Нормализующий поток**  
Метод, который с помощью последовательности обратимых преобразований превращает простое распределение в сложное, пригодное для генеративного моделирования.

**Invertible Neural Network – Обратимая нейронная сеть**  
Архитектура, обеспечивающая возможность обратного прохождения данных, что необходимо для работы нормализующих потоков.

**Latent Variable Model – Модель скрытых переменных**  
Подход, использующий скрытые переменные для представления сложных распределений данных, часто применяемый в генеративных моделях.

**KL Divergence – Расхождение Кульбака-Лейблера**  
Мера различия между двумя распределениями, используемая для регуляризации и оптимизации в VAE.

**Flow-based Generative Model – Генеративная модель на основе потоков**  
Модель, генерирующая данные путем последовательного применения обратимых преобразований к простому распределению.

**Autoregressive Flows – Авторегрессивные потоки**  
Потоковые модели, где каждое преобразование зависит от предыдущих выходов, обеспечивая более гибкую генерацию данных.

**Stochastic Variational Inference – Стохастический вариационный вывод**  
Метод приближенного вывода в сложных вероятностных моделях с использованием стохастической оптимизации.

**Bayesian Inference – Байесовский вывод**  
Метод вычисления апостериорного распределения параметров модели с использованием априорных знаний и наблюдаемых данных.

**Posterior Approximation – Аппроксимация апостериорного распределения**  
Процесс приближения истинного апостериорного распределения с помощью более простого, вычислимого распределения.

**Amortized Inference – Амортизированный вывод**  
Использование нейронной сети для быстрого вычисления параметров аппроксимирующего распределения для различных входов, что ускоряет вывод.

**Flow-based Inference – Потоковый вывод**  
Применение нормализующих потоков для моделирования сложных распределений напрямую через обратимые преобразования.

**Variational Lower Bound – Вариационная нижняя граница**  
Альтернативное название Evidence Lower Bound (ELBO), используемое для оптимизации вариационных моделей.

---

## Recommender Systems

**Collaborative Filtering – Коллаборативная фильтрация**  
Метод рекомендаций, основанный на анализе взаимодействий пользователей с объектами для выявления сходств и предпочтений.

**Content-Based Filtering – Фильтрация на основе контента**  
Подход, рекомендующий объекты на основе их характеристик и сходства с предпочтениями пользователя.

**Matrix Factorization – Факторизация матриц**  
Метод разложения матрицы взаимодействий для выявления скрытых факторов, определяющих предпочтения пользователей.

**Neural Collaborative Filtering – Нейронная коллаборативная фильтрация**  
Применение нейронных сетей для моделирования сложных нелинейных зависимостей в данных рекомендаций.

**Implicit Feedback – Неявная обратная связь**  
Использование косвенных сигналов (например, клики, просмотры) для формирования рекомендаций в отсутствие явных оценок.

**Cold Start Problem – Проблема холодного старта**  
Задача предоставления рекомендаций новым пользователям или объектам при отсутствии достаточного объёма данных.

**Hybrid Recommender System – Гибридная рекомендательная система**  
Система, объединяющая несколько методов рекомендаций для повышения точности и стабильности выводов.

**Ranking Loss – Функция ранжирования**  
Функция потерь, оптимизирующая порядок рекомендаций для улучшения качества выдачи.

**Session-based Recommendations – Рекомендации на основе сессий**  
Подход, формирующий рекомендации, исходя из текущей активности пользователя в рамках одной сессии.

**User Embeddings – Встраивания пользователей**  
Непрерывные представления пользователей, извлекаемые из их взаимодействий, для улучшения персонализации рекомендаций.

**Implicit Matrix Factorization – Факторизация матриц на неявных данных**  
Метод разложения матрицы взаимодействий, использующий неявные сигналы (например, клики, просмотры) для выявления скрытых факторов.

**Context-Aware Recommendations – Рекомендации с учётом контекста**  
Системы, которые учитывают дополнительную информацию о пользователях и окружающей среде при формировании рекомендаций.

**Graph-based Recommenders – Графовые рекомендательные системы**  
Подход, использующий графовые структуры для моделирования отношений между пользователями и объектами, что улучшает персонализацию.

**Bayesian Personalized Ranking (BPR) – Байесовский персонализированный ранжир**  
Метод оптимизации для рекомендательных систем, нацеленный на ранжирование предпочтительных объектов выше непредпочтительных.

**Session-based Collaborative Filtering – Коллаборативная фильтрация на основе сессий**  
Подход, учитывающий временные последовательности действий пользователей для улучшения рекомендаций в реальном времени.

---

## Robotics and Control

**Inverse Kinematics – Обратная кинематика**  
Расчёт углов сочленений, необходимых для достижения заданной позиции конечности робота.

**Model Predictive Control (MPC) – Предиктивное управление**  
Метод оптимизации последовательности управляющих сигналов на основе модели динамики системы.

**Imitation Learning – Обучение имитации**  
Метод, при котором агент учится копировать поведение эксперта, используя записи его действий.

**Sim2Real – Переход из симуляции в реальность**  
Процесс адаптации моделей, обученных в симуляторе, для эффективной работы в реальных условиях.

**Trajectory Optimization – Оптимизация траектории**  
Процесс нахождения оптимального пути для достижения цели с минимальными затратами ресурсов.

**End-to-End Control – Конечное управление**  
Подход, при котором система напрямую преобразует входные данные (например, видео) в управляющие сигналы без промежуточной обработки.

**Sensor Fusion – Слияние данных с датчиков**  
Объединение информации с различных датчиков для получения более точного и надёжного восприятия окружающей среды.

**SLAM (Simultaneous Localization and Mapping) – Одновременная локализация и картографирование**  
Алгоритмы, позволяющие роботу одновременно определять своё местоположение и строить карту окружающего пространства.

**Reinforcement Learning for Robotics – Обучение с подкреплением в робототехнике**  
Применение методов RL для обучения роботов сложным задачам управления и взаимодействия с окружением.

**Control Theory – Теория управления**  
Научная дисциплина, изучающая принципы и методы регулирования динамических систем для достижения заданных целей.

**Forward Kinematics – Прямая кинематика**  
Метод вычисления положения конечностей робота на основе заданных углов сочленений, противоположный обратной кинематике.

**Dynamic Modeling – Динамическое моделирование**  
Создание математической модели движения робота с учётом сил, моментов и инерционных характеристик.

**Force Feedback – Обратная связь по силе**  
Система, позволяющая роботу ощущать прикладываемые силы для корректировки своих действий и обеспечения безопасности.

**Impedance Control – Импедансное управление**  
Метод управления, регулирующий динамические характеристики робота (например, жесткость, демпфирование) для безопасного взаимодействия с окружением.

**Visual Servoing – Визуальное сервирование**  
Метод управления, при котором позиция или ориентация робота корректируется на основе визуальной обратной связи с камер.

**Compliance Control – Управление по податливости**  
Стратегия, позволяющая роботу адаптироваться к внешним силам, снижая риск повреждения оборудования.

**Path Planning – Планирование пути**  
Алгоритмы, определяющие оптимальный маршрут движения робота с учётом препятствий и динамических ограничений.

**Force/Torque Sensing – Датчики силы/момента**  
Аппаратные средства, измеряющие силы и моменты, действующие на робота, для обратной связи в системах управления.

---

## Interpretability and Explainability

**Interpretability – Интерпретируемость**  
Способность модели предоставлять понятные объяснения своих решений, что повышает доверие пользователей и облегчает диагностику ошибок.

**Saliency Map – Карта важности**  
Визуальное представление, выделяющее области входного изображения, наиболее значимые для принятия решения моделью, обычно вычисляемое с использованием градиентов.

**LIME (Local Interpretable Model-agnostic Explanations) – Локально интерпретируемые объяснения, не зависящие от модели**  
Метод, который объясняет отдельные предсказания, аппроксимируя сложную модель локально интерпретируемой, более простой моделью.

**SHAP (SHapley Additive exPlanations) – SHAP объяснения**  
Метод, основанный на теории игр, который назначает каждому признаку значение его вклада в предсказание, объединяя локальные объяснения в единое целое.

**Integrated Gradients – Интегрированные градиенты**  
Метод атрибуции, который вычисляет вклад каждого входного признака в предсказание, интегрируя градиенты вдоль пути от базового значения к исходному входу.

**Grad-CAM (Gradient-weighted Class Activation Mapping) – Градиентное взвешенное отображение активаций классов**  
Техника визуализации, использующая градиенты, поступающие к последнему сверточному слою, для создания карты, показывающей, какие области изображения наиболее влияют на предсказание.

**Layer-wise Relevance Propagation (LRP) – Пропаганда релевантности по слоям**  
Метод, который распространяет предсказание модели назад по слоям, назначая релевантность каждому входному признаку, что позволяет объяснить, как отдельные части входа влияют на итоговый вывод.

**Counterfactual Explanations – Контрфактические объяснения**  
Метод, который показывает, какие минимальные изменения во входных данных могли бы изменить предсказание модели, помогая понять границы принятия решений.

**Partial Dependence Plot (PDP) – График частичной зависимости**  
Визуализация, демонстрирующая влияние одного или нескольких признаков на предсказание модели, усредняя влияние остальных признаков.

**Individual Conditional Expectation (ICE) – Индивидуальные графики условного ожидания**  
Метод, похожий на PDP, но отображающий эффекты для каждого наблюдения отдельно, что позволяет увидеть вариации влияния признаков между различными экземплярами.

**Surrogate Models – Суррогатные модели**  
Простые, интерпретируемые модели, которые аппроксимируют работу сложной модели локально или глобально, что позволяет объяснить её поведение.

**Attention Visualization – Визуализация внимания**  
Методы, демонстрирующие веса внимания в моделях, использующих механизм внимания, что помогает понять, на какие части входа модель обращает внимание при принятии решения.

**Model-Agnostic Explanations – Независимые от модели объяснения**  
Методы, которые могут применяться к любой модели без знания её внутренней структуры, например, LIME или SHAP, позволяя объяснять предсказания вне зависимости от архитектуры.

**Feature Importance – Важность признаков**  
Оценка вклада каждого входного признака в предсказание модели, которая может вычисляться с использованием различных методов, таких как перестановочная важность или значения SHAP.

**Global vs. Local Explanations – Глобальные и локальные объяснения**  
Глобальные объяснения описывают общее поведение модели, в то время как локальные объяснения фокусируются на отдельных предсказаниях. Это позволяет понять как общие тенденции, так и индивидуальные решения модели.

**Concept Activation Vectors (CAVs) – Векторы активации концепций**  
Метод, позволяющий оценить влияние высокоуровневых концепций, понятных человеку, на предсказания модели, что помогает выявлять потенциальные предвзятости и зависимости.

---

## Edge AI and Deployment

**Model Compression – Сжатие модели**  
Процессы уменьшения размера модели (например, через обрезку, квантование, дистилляцию) для ускорения вывода и снижения потребления ресурсов.

**Inference Optimization – Оптимизация вывода**  
Методы повышения эффективности выполнения модели на этапе инференса, включая оптимизацию памяти и вычислительных операций.

**TensorRT – TensorRT**  
Фреймворк NVIDIA для оптимизации и ускорения вывода нейронных сетей на GPU.

**ONNX (Open Neural Network Exchange) – ONNX**  
Открытый формат для представления нейронных сетей, позволяющий переносить модели между различными фреймворками.

**Latency – Задержка**  
Время, необходимое для получения ответа модели, критически важное для приложений в реальном времени.

**Edge Computing – Периферийные вычисления**  
Выполнение вычислительных задач непосредственно на устройствах или вблизи источника данных для снижения задержек и повышения эффективности.

**Hardware Acceleration – Аппаратное ускорение**  
Использование специализированного оборудования (GPU, TPU, FPGA) для повышения производительности вычислений.

**Model Quantization – Квантование модели**  
Преобразование параметров модели в более компактный формат с пониженной точностью для ускорения инференса.

**Containerization – Контейнеризация**  
Технология упаковки приложений и их зависимостей в изолированные контейнеры для облегчения развертывания и масштабирования.

**Deployment Pipeline – Пайплайн развертывания**  
Набор процессов и инструментов для автоматизации развертывания моделей в продакшене, обеспечивающий их надёжное обновление и масштабирование.

---

## Additional Topics and Methods

**Self-Supervised Learning – Самостоятельное обучение**  
Метод, при котором модель обучается на неразмеченных данных, извлекая скрытые закономерности, что позволяет использовать огромные объемы информации для предварительного обучения.

**Meta-Learning – Мета-обучение**  
Подход, позволяющий моделям быстро адаптироваться к новым задачам с минимальным количеством примеров, развивая умение «учиться учиться».

**Automated Machine Learning (AutoML) – Автоматизированное машинное обучение**  
Процесс автоматизации выбора моделей, гиперпараметров и этапов предобработки данных для оптимизации процесса обучения без участия эксперта.

**Adversarial Attacks – Атаки с использованием противоречивых примеров**  
Методы создания специально сформированных входных данных, способных обмануть модель, что позволяет выявить и устранить её уязвимости.

**Robustness – Надёжность**  
Способность модели сохранять работоспособность при изменениях входных данных или внешних условиях, что критически важно для практического применения.

**Interpretability – Интерпретируемость**  
Способность модели предоставлять понятные объяснения своих решений, что повышает доверие пользователей и облегчает диагностику ошибок.

**Explainability – Объяснимость**  
Процесс разработки инструментов и методов, позволяющих продемонстрировать причины, по которым модель принимает те или иные решения.

**Continual Learning – Непрерывное обучение**  
Методика, позволяющая модели адаптироваться к новым данным без утраты ранее полученных знаний, что особенно важно в динамичных средах.

**Few-Shot Meta-Learning – Мета-обучение с малым количеством примеров**  
Подход, позволяющий моделям эффективно обучаться на ограниченном объёме данных, используя накопленный ранее опыт.

**Data Imbalance Handling – Работа с несбалансированными данными**  
Набор техник для корректного обучения на выборках с неравномерным распределением классов, что помогает улучшить точность для редких категорий.

**Causal Inference in DL – Причинный вывод в глубоких моделях**  
Методы выявления и анализа причинно-следственных связей в данных с использованием глубокого обучения для понимания истинных взаимосвязей.

**Neural Tangent Kernel (NTK) – Ядерный подход нейронных касательных**  
Теоретическая модель, позволяющая анализировать поведение бесконечно широких нейронных сетей, что помогает лучше понять динамику оптимизации.

**Hyperparameter Optimization – Оптимизация гиперпараметров**  
Методы автоматического или полуавтоматического поиска наилучших значений гиперпараметров модели для достижения оптимальной производительности.

**Neural Architecture Search (NAS) – Поиск нейронной архитектуры**  
Метод автоматизированного поиска оптимальной структуры нейронной сети, позволяющий находить эффективные архитектурные решения без ручного проектирования.

**Federated Learning – Федерированное обучение**  
Подход, при котором модель обучается на распределённых данных, находящихся на устройствах пользователей, без централизованного хранения, что улучшает приватность данных.

**Explainable AI (XAI) – Объяснимая ИИ**  
Область исследований и практик, направленных на создание моделей, решения которых можно интерпретировать и объяснять конечному пользователю.

**Adversarial Training – Адаптация к атакам**  
Методика обучения модели на противоречивых примерах для повышения её устойчивости к атакующим воздействиям, что улучшает безопасность системы.

**Transfer Learning in RL – Трансферное обучение в обучении с подкреплением**  
Использование знаний, полученных в одной задаче, для ускорения обучения в другой задаче.

**Self-Play – Самоигра**  
Метод обучения, при котором агент учится, играя против самого себя, что часто используется в стратегических играх.

**Curriculum Learning – Обучение по учебному плану**  
Метод постепенного увеличения сложности задач в процессе обучения, что помогает агенту осваивать сложные задачи шаг за шагом.
