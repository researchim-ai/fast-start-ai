# Словарь современного Deep Learning

## Общий DL

**Deep Learning – Глубокое обучение**  
Область машинного обучения, использующая нейронные сети с множеством слоёв для решения сложных задач. Эта методика позволяет моделям автоматически извлекать высокоуровневые абстракции из данных, что делает её эффективной для распознавания образов, обработки естественного языка и других сложных задач.

**Neural Network – Нейронная сеть**  
Модель, состоящая из взаимосвязанных нейронов (узлов), имитирующих работу человеческого мозга для обработки информации. Нейронные сети способны моделировать сложные зависимости между входными и выходными данными, что делает их ключевым элементом в решении задач классификации, регрессии и генерации данных.

**Feedforward Neural Network – Прямая нейронная сеть**  
Базовый тип сети, в которой данные проходят от входного слоя к выходному без обратных связей. Такие сети используются для задач, где важна прямая связь между входом и выходом, без учета временных зависимостей.

**Perceptron – Перцептрон**  
Элементарная модель нейрона, принимающая входные данные, обрабатывающая их и выдающая результат. Перцептрон демонстрирует принцип линейной классификации и служит фундаментом для построения более сложных архитектур.

**Fully Connected Layer – Полносвязный слой**  
Слой, в котором каждый нейрон соединён с каждым нейроном предыдущего слоя. Он позволяет моделям обучаться сложным нелинейным зависимостям за счёт оптимизации большого числа весовых коэффициентов.

**Activation Function – Функция активации**  
Функция, применяемая к сумме взвешенных входов нейрона для введения нелинейности (например, ReLU, sigmoid, tanh). Нелинейность позволяет сети моделировать сложные зависимости, выходящие за рамки линейных отношений.

**Softmax – Софтмакс**  
Функция, преобразующая вектор значений в вероятностное распределение, часто используемая для многоклассовой классификации. Она нормализует выходы так, чтобы их сумма равнялась единице, что позволяет интерпретировать их как вероятности.

**Loss Function – Функция потерь**  
Мера ошибки, определяющая разницу между предсказанными и истинными значениями, которую модель стремится минимизировать. Функция потерь направляет процесс оптимизации, позволяя корректировать веса сети на основе величины ошибки.

**Gradient Descent – Метод градиентного спуска**  
Алгоритм оптимизации для корректировки весов модели в направлении наискорейшего уменьшения функции потерь. Метод итеративно обновляет параметры, используя вычисление производных, чтобы найти минимум функции потерь.

**Momentum – Моментум**  
Метод ускорения сходимости оптимизации за счёт учёта предыдущих обновлений весов для сглаживания траектории. Это помогает преодолеть локальные минимумы и ускоряет обучение глубоких моделей.

**Adam – Адам**  
Популярный алгоритм оптимизации, объединяющий адаптивное вычисление моментов и метод моментума для ускорения сходимости. Он автоматически настраивает скорость обучения для каждого параметра, что делает его универсальным и эффективным.

**Backpropagation – Обратное распространение ошибки**  
Метод расчёта градиентов, используемый для обновления весов нейронной сети. Он распространяет ошибку от выходного слоя к входному, позволяя корректировать веса на основе вклада каждого нейрона в итоговую ошибку.

**Optimizer – Оптимизатор**  
Алгоритм (например, SGD, Adam, RMSprop), используемый для обновления параметров модели в процессе обучения. Выбор оптимизатора влияет на скорость сходимости и качество итоговой модели.

**Regularization – Регуляризация**  
Набор техник для уменьшения переобучения модели, таких как L1, L2, Dropout и Early Stopping. Регуляризация добавляет ограничения к функции потерь или структуре сети, улучшая обобщающие способности модели.

**Dropout – Отбрасывание**  
Метод регуляризации, при котором случайным образом исключаются некоторые нейроны во время обучения. Это помогает предотвратить избыточное запоминание обучающих данных и улучшает способность модели обобщать.

**Batch Normalization – Нормализация пакета**  
Техника, нормализующая входы каждого слоя по мини-пакетам данных для стабилизации и ускорения обучения. Нормализация снижает влияние изменений распределения входных данных, что способствует быстрой и стабильной сходимости.

**Learning Rate – Скорость обучения**  
Гиперпараметр, определяющий величину шага при обновлении весов модели. Правильный выбор скорости обучения критически важен: слишком высокий может привести к нестабильности, а слишком низкий — к медленной сходимости.

**Learning Rate Scheduler – Планировщик скорости обучения**  
Механизм динамического изменения скорости обучения в процессе тренировки, позволяющий модели тонко настраиваться на поздних этапах оптимизации.

**Epoch – Эпоха**  
Один полный проход по всему набору тренировочных данных, после которого происходит обновление параметров модели на основе полной выборки.

**Mini-Batch – Мини-пакет**  
Небольшая часть обучающих данных, используемая для одного шага обновления весов. Разбиение данных на мини-пакеты уменьшает вычислительную нагрузку и способствует стабильности обучения.

**Weight Initialization – Инициализация весов**  
Процесс задания начальных значений параметров нейронной сети, что влияет на скорость и качество обучения. Хорошая инициализация помогает избежать проблем с затухающими или взрывающимися градиентами.

**Transfer Learning – Трансферное обучение**  
Метод адаптации модели, предварительно обученной на одном наборе данных, для решения смежной задачи. Это позволяет использовать уже извлечённые признаки и значительно сокращает время обучения.

**Model Pruning – Обрезка модели**  
Процесс удаления избыточных параметров для уменьшения размера модели и повышения её вычислительной эффективности. Обрезка снижает сложность модели без значительной потери точности.

**Quantization – Квантование**  
Снижение точности представления параметров модели с целью ускорения вычислений и экономии памяти, что особенно важно для развертывания моделей на мобильных устройствах.

**Ensemble Methods – Ансамблевые методы**  
Подход, при котором несколько моделей объединяются для получения более точных и устойчивых предсказаний. Ансамблирование позволяет компенсировать ошибки отдельных моделей и улучшать обобщение.

**Cross-Validation – Кросс-валидация**  
Метод оценки обобщающих способностей модели путём разбиения данных на несколько обучающих и тестовых подмножеств, что помогает выявить переобучение.

**Residual Network (ResNet) – Остаточная сеть**  
Архитектура с пропускными связями, позволяющими строить очень глубокие сети без проблем затухающего градиента. Пропуски позволяют передавать информацию напрямую через несколько слоёв.

**Graph Neural Network (GNN) – Графовая нейронная сеть**  
Модель для обработки структурированных данных в виде графов, где связи между объектами играют ключевую роль. GNN используются в задачах социальных сетей, молекулярной химии и других областях.

**Capsule Network – Капсульная сеть**  
Архитектура, сохраняющая пространственные иерархии между объектами, что полезно для задач распознавания изображений. Капсульные сети передают информацию о позиции и ориентации объектов, повышая устойчивость к трансформациям.

**DropConnect – Отбрасывание связей**  
Метод регуляризации, при котором случайным образом исключаются отдельные веса вместо целых нейронов, что улучшает обобщающую способность модели.

**Sparse Coding – Разреженное кодирование**  
Техника представления данных с использованием ограниченного числа активных элементов, способствующая выявлению наиболее значимых признаков.

**Overfitting – Переобучение**  
Ситуация, когда модель слишком хорошо запоминает обучающие данные, теряя способность обобщать на новые примеры. Способы борьбы включают регуляризацию и аугментацию данных.

**Underfitting – Недообучение**  
Состояние, при котором модель не способна уловить основные закономерности в данных, что приводит к низкой точности как на обучающем, так и на тестовом наборах.

**Early Stopping – Раннее прекращение обучения**  
Метод остановки обучения, когда качество модели на валидационном наборе перестаёт улучшаться. Это помогает предотвратить переобучение.

**Gradient Clipping – Ограничение градиента**  
Техника, предотвращающая взрыв градиентов при обучении глубоких сетей, ограничивая максимальное значение градиента.

**Data Normalization – Нормализация данных**  
Процесс приведения данных к единому масштабу, что улучшает сходимость модели и повышает стабильность обучения.

---

## LLM — большие языковые модели

**LLM (Large Language Model) – Большая языковая модель**  
Комплексная нейросеть, обученная на огромном количестве текстовых данных для генерации, анализа и понимания естественного языка. Такие модели способны улавливать тонкости языка, контекст и смысл, что позволяет им выполнять широкий спектр задач – от перевода до создания творческих текстов.

**Language Modeling – Языковое моделирование**  
Задача предсказания следующего слова или символа в последовательности, лежащая в основе обучения языковых моделей. Этот процесс помогает модели понять структуру языка и взаимосвязи между словами.

**Pretraining – Предобучение**  
Этап обучения модели на большом корпусе текстов без специализированной разметки, позволяющий освоить базовые языковые закономерности до тонкой настройки на конкретную задачу.

**Masked Language Modeling – Замаскированное языковое моделирование**  
Метод, при котором часть токенов заменяется специальными масками, а модель обучается восстанавливать пропущенные слова, используя контекст окружающих токенов.

**Causal Language Modeling – Причинное языковое моделирование**  
Обучение модели предсказывать следующий токен на основе уже сгенерированного текста, что характерно для авторегрессионных моделей.

**Fine-tuning – Тонкая настройка**  
Доработка предобученной модели на специализированном наборе данных для повышения точности решения конкретной задачи, будь то перевод, суммирование или классификация.

**Parameter Efficient Fine-Tuning – Эффективное дообучение**  
Методы адаптации крупных моделей с минимальными изменениями параметров (например, LoRA), позволяющие экономить вычислительные ресурсы при сохранении высокой точности.

**Token – Токен**  
Минимальная единица текста (слово, часть слова или символ), с которой работает модель. Токенизация помогает разбивать текст на понятные единицы для дальнейшей обработки.

**Prompt – Промт/Запрос/Подсказка**  
Входной текст, подаваемый модели для генерации ответа, от которого зависит качество и релевантность результата.

**Context Window – Контекстное окно**  
Ограничение на количество токенов, которое модель может одновременно учитывать, влияющее на «память» и способность учитывать длинные зависимости в тексте.

**In-context Learning – Обучение в контексте**  
Способность модели адаптироваться к задаче на основе примеров, приведённых непосредственно в запросе, без дополнительного обучения.

**Zero-shot Learning – Обучение без примеров**  
Возможность модели выполнять задачу на основе только описания без предоставления примеров, демонстрируя высокий уровень обобщения.

**Few-shot Learning – Обучение с несколькими примерами**  
Метод, при котором в запросе предоставляется несколько примеров для улучшения понимания задачи моделью и повышения точности вывода.

**Chain-of-thought Reasoning – Цепочка рассуждений**  
Техника пошагового описания процесса решения задачи, позволяющая отследить логику, по которой модель приходит к ответу, что полезно для диагностики.

**Decoding – Декодирование**  
Процесс генерации текста из скрытых представлений модели; выбор алгоритма (например, жадный поиск или beam search) влияет на качество и разнообразие сгенерированного текста.

**Temperature – Температура**  
Гиперпараметр, регулирующий степень случайности в процессе генерации текста: низкое значение даёт более детерминированный вывод, а высокое – повышает креативность.

**Beam Search – Поиск по лучу**  
Алгоритм, одновременно рассматривающий несколько вариантов продолжения текста для выбора наиболее вероятного, что улучшает качество сгенерированного результата.

**Knowledge Distillation – Дистилляция знаний**  
Процесс переноса знаний от большой модели к меньшей, позволяющий сократить вычислительные затраты без значительной потери точности.

**Multitask Learning – Мультитаскинг**  
Обучение модели выполнению нескольких задач одновременно, что способствует улучшению обобщающих способностей за счёт использования взаимосвязанных задач.

**Model Scaling – Масштабирование модели**  
Увеличение числа параметров и вычислительной мощности модели для повышения её производительности, что часто приводит к улучшению качества предсказаний.

**Attention Mask – Маска внимания**  
Механизм, определяющий, какие токены учитывать при вычислении внимания, что помогает избежать влияния паддинга и нерелевантной информации.

**Positional Encoding – Позиционное кодирование**  
Метод добавления информации о положении токенов в последовательности для сохранения порядка слов, что критично для понимания контекста.

**Pretraining Objectives – Цели предобучения**  
Набор задач (например, Next Sentence Prediction, Permuted Language Modeling) для обучения базовым языковым закономерностям до тонкой настройки модели.

**Scaling Laws – Законы масштабирования**  
Эмпирические зависимости, описывающие, как увеличение размера модели и объёма данных влияет на её производительность и точность.

**Multilingual Modeling – Мультиязычное моделирование**  
Подход, позволяющий обучать модели, способные работать с несколькими языками, что расширяет их применение на глобальном уровне.

**Next Sentence Prediction – Предсказание следующего предложения**  
Задача, в которой модель определяет, является ли следующее предложение логически связанным с предыдущим, что помогает уловить структуру текста.

**Text Summarization – Резюмирование текста**  
Процесс создания краткого изложения основного содержания документа, что полезно для автоматического создания аннотаций и обзоров.

**Prompt Tuning – Тонкая настройка запросов**  
Метод оптимизации формулировки входных запросов для улучшения качества и релевантности ответов модели.

**Encoder-Decoder Model – Модель энкодер-декодер**  
Архитектура, состоящая из двух компонентов: энкодера для преобразования входного текста в скрытое представление и декодера для генерации ответа.

**Autoregressive Model – Авторегрессионная модель**  
Модель, генерирующая текст последовательно, предсказывая следующий токен на основе уже сгенерированных, что обеспечивает связность и непрерывность вывода.

---

## Computer Vision

**Image Classification – Классификация изображений**  
Задача определения категории, к которой принадлежит изображение, посредством анализа визуальных признаков с использованием специализированных моделей.

**Object Detection – Обнаружение объектов**  
Процесс локализации и классификации объектов на изображении с помощью рамок или масок, позволяющий определять как класс, так и положение объектов.

**Semantic Segmentation – Семантическая сегментация**  
Разбиение изображения на области, соответствующие различным классам, где каждому пикселю присваивается определённый класс без выделения отдельных экземпляров.

**Instance Segmentation – Сегментация объектов**  
Метод, позволяющий выделять отдельные объекты одного класса и определять их индивидуальные границы в рамках одного изображения.

**Convolutional Neural Network (CNN) – Сверточная нейронная сеть**  
Архитектура, оптимизированная для обработки изображений с помощью свёрток, что позволяет эффективно извлекать пространственные признаки.

**Convolution – Свёртка**  
Операция, применяемая для извлечения локальных признаков из изображения посредством применения фильтров, выявляющих границы, текстуры и формы.

**Pooling – Подвыборка**  
Операция уменьшения пространственных размеров представлений (например, max pooling или average pooling) для выделения наиболее устойчивых признаков и снижения вычислительной нагрузки.

**Feature Map – Карта признаков**  
Результат применения свёрточного фильтра к изображению, содержащий обнаруженные визуальные признаки, которые используются для последующего анализа.

**Image Preprocessing – Предобработка изображений**  
Методы подготовки изображений к анализу, включая нормализацию, изменение размера и аугментацию, что улучшает качество обучения модели.

**Image Augmentation – Аугментация изображений**  
Методы искусственного увеличения объёма данных посредством применения трансформаций (поворот, масштабирование, отражение), что помогает избежать переобучения.

**Edge Detection – Выделение контуров**  
Алгоритмы для обнаружения границ объектов, способствующие выделению структурных особенностей изображения, важных для анализа формы.

**Optical Flow – Оптический поток**  
Метод оценки движения объектов в последовательности изображений или видеопотоке, позволяющий отслеживать динамику и скорость перемещений.

**Pose Estimation – Определение позы**  
Задача определения положения и ориентации человека или объекта на изображении, используемая в системах распознавания действий и анимации.

**Generative Adversarial Networks (GAN) в CV – Генеративно-состязательные сети**  
Применение GAN для генерации реалистичных изображений, улучшения качества или стилизации, где генератор и дискриминатор обучаются в противоборстве.

**Visual Transformers – Визуальные трансформеры**  
Адаптация архитектуры трансформеров для обработки изображений, где механизм внимания помогает выделять ключевые признаки на разных масштабах.

**Region Proposal Network (RPN) – Сеть предложений регионов**  
Компонент для генерации кандидатов областей на изображении, потенциально содержащих объекты, что ускоряет процесс обнаружения.

**Feature Pyramid Network (FPN) – Сеть пирамиды признаков**  
Архитектура, объединяющая признаки на различных масштабах для улучшения обнаружения объектов разного размера.

**Style Transfer – Перенос стиля**  
Метод, позволяющий перенести художественный стиль одного изображения на контент другого, создавая новые визуальные эффекты.

**Super-Resolution – Сверхразрешение**  
Техника повышения разрешения изображения с восстановлением утраченных деталей, используемая для улучшения качества низкоразрешённых изображений.

**Image Denoising – Удаление шума**  
Методы очистки изображений от случайных искажений и артефактов, что улучшает качество входных данных для дальнейшей обработки.

**Image Inpainting – Восстановление изображений**  
Процесс заполнения пропущенных или повреждённых частей изображения на основе окружающих пикселей, используемый для реставрации и редактирования.

**3D Reconstruction – 3D реконструкция**  
Методы создания трёхмерной модели объекта или сцены на основе одного или нескольких изображений, что важно для компьютерной графики и робототехники.

**Object Tracking – Отслеживание объектов**  
Технологии, позволяющие непрерывно определять положение и перемещения объектов в видеопотоке, что важно для видеонаблюдения и автономного вождения.

**Optical Character Recognition (OCR) – Оптическое распознавание символов**  
Методы преобразования изображений с текстом в редактируемый текстовый формат, широко используемые для автоматизации ввода данных.

---

## Reinforcement Learning

**Reinforcement Learning – Обучение с подкреплением**  
Область, в которой агент обучается на основе наград и штрафов, получаемых от взаимодействия с окружающей средой. Такой подход позволяет моделям самостоятельно изучать оптимальные стратегии на основе опыта.

**Q-Learning – Q-обучение**  
Метод, при котором агент изучает оптимальную политику, оценивая Q-функцию — ожидаемую награду для пары состояние-действие, что помогает выбирать наиболее выгодные действия.

**Deep Q-Network (DQN) – Глубокая Q-сеть**  
Комбинация Q-обучения и нейронных сетей, позволяющая работать с высокоразмерными пространствами состояний и применять обучение с подкреплением в сложных средах.

**Policy Gradient – Градиент политики**  
Метод оптимизации, при котором параметры политики обновляются напрямую для максимизации ожидаемой награды, что позволяет агенту обучаться стратегии без явной оценки всех вариантов.

**Actor-Critic – Актор-критик**  
Подход, где «актор» принимает решения, а «критик» оценивает их, помогая корректировать стратегию. Такой метод сочетает преимущества оценки ценности и прямой оптимизации политики.

**Proximal Policy Optimization (PPO) – Проксимальное оптимальное обучение политики**  
Алгоритм, обеспечивающий стабильное обновление политики посредством ограничения изменений, что помогает избежать резких скачков в обучении.

**Monte Carlo Tree Search (MCTS) – Монте-Карло поиск по дереву**  
Метод планирования, использующий симуляцию множества вариантов развития событий для выбора оптимальных действий, широко применяемый в стратегических играх.

**Exploration vs. Exploitation – Исследование vs. использование**  
Баланс между поиском новых стратегий (исследование) и применением уже известных для максимизации награды, критически важный для успешного обучения.

**SARSA – SARSA алгоритм**  
Метод обучения с подкреплением, где обновление производится на основе текущего действия и следующего шага, отражая реальное поведение агента.

**Double DQN – Двойная глубокая Q-сеть**  
Улучшенная версия DQN, снижающая переоценку Q-значений за счёт двойного оценивания, что обеспечивает более стабильное обучение.

**Prioritized Experience Replay – Приоритетный повтор опыта**  
Метод, позволяющий чаще использовать для обучения наиболее значимые переходы, ускоряя процесс обучения за счёт фокусировки на ошибках.

**Reward Shaping – Формирование награды**  
Методика модификации функции награды для направления агента к желаемому поведению, что помогает ускорить обучение.

**Discount Factor – Коэффициент дисконтирования**  
Параметр, определяющий важность будущих наград по сравнению с текущими, влияющий на стратегию агента в долгосрочной перспективе.

**Exploration Strategies – Стратегии исследования**  
Набор методов (например, ε-greedy, Boltzmann exploration) для обеспечения баланса между исследованием новых действий и использованием известных выгодных стратегий.

**Eligibility Traces – Следы пригодности**  
Механизм, позволяющий учитывать влияние предыдущих состояний и действий при обновлении политики, что ускоряет обучение в задачах с разреженной наградой.

**Policy Regularization – Регуляризация политики**  
Методы ограничения изменений в политике для предотвращения чрезмерных колебаний в стратегии агента, способствующие стабильному обучению.

---

## Additional Topics and Methods

**Self-Supervised Learning – Самостоятельное обучение**  
Метод, при котором модель обучается на неразмеченных данных, извлекая скрытые закономерности, что позволяет использовать огромные объемы информации для предварительного обучения.

**Meta-Learning – Мета-обучение**  
Подход, позволяющий моделям быстро адаптироваться к новым задачам с минимальным количеством примеров, развивая умение «учиться учиться».

**Automated Machine Learning (AutoML) – Автоматизированное машинное обучение**  
Процесс автоматизации выбора моделей, гиперпараметров и этапов предобработки данных для оптимизации процесса обучения без участия эксперта.

**Adversarial Attacks – Атаки с использованием противоречивых примеров**  
Методы создания специально сформированных входных данных, способных обмануть модель, что позволяет выявить и устранить её уязвимости.

**Robustness – Надёжность**  
Способность модели сохранять работоспособность при изменениях входных данных или внешних условиях, что критически важно для практического применения.

**Interpretability – Интерпретируемость**  
Способность модели предоставлять понятные объяснения своих решений, что повышает доверие пользователей и облегчает диагностику ошибок.

**Explainability – Объяснимость**  
Процесс разработки инструментов и методов, позволяющих продемонстрировать причины, по которым модель принимает те или иные решения.

**Continual Learning – Непрерывное обучение**  
Методика, позволяющая модели адаптироваться к новым данным без утраты ранее полученных знаний, что особенно важно в динамичных средах.

**Few-Shot Meta-Learning – Мета-обучение с малым количеством примеров**  
Подход, позволяющий моделям эффективно обучаться на ограниченном объёме данных, используя накопленный ранее опыт.

**Data Imbalance Handling – Работа с несбалансированными данными**  
Набор техник для корректного обучения на выборках с неравномерным распределением классов, что помогает улучшить точность для редких категорий.

**Causal Inference in DL – Причинный вывод в глубоких моделях**  
Методы выявления и анализа причинно-следственных связей в данных с использованием глубокого обучения для понимания истинных взаимосвязей.

**Neural Tangent Kernel (NTK) – Ядерный подход нейронных касательных**  
Теоретическая модель, позволяющая анализировать поведение бесконечно широких нейронных сетей, что помогает лучше понять динамику оптимизации.

**Hyperparameter Optimization – Оптимизация гиперпараметров**  
Методы автоматического или полуавтоматического поиска наилучших значений гиперпараметров модели для достижения оптимальной производительности.

**Neural Architecture Search (NAS) – Поиск нейронной архитектуры**  
Метод автоматизированного поиска оптимальной структуры нейронной сети, позволяющий находить эффективные архитектурные решения без ручного проектирования.

**Federated Learning – Федерированное обучение**  
Подход, при котором модель обучается на распределённых данных, находящихся на устройствах пользователей, без централизованного хранения, что улучшает приватность данных.

**Explainable AI (XAI) – Объяснимая ИИ**  
Область исследований и практик, направленных на создание моделей, решения которых можно интерпретировать и объяснять конечному пользователю.

**Adversarial Training – Адаптация к атакам**  
Методика обучения модели на противоречивых примерах для повышения её устойчивости к атакующим воздействиям, что улучшает безопасность системы.
