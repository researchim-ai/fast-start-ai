# Словарь современного Deep Learning

## Общий DL

**Deep Learning – Глубокое обучение**  
Область машинного обучения, использующая нейронные сети с множеством слоёв для решения сложных задач.

**Neural Network – Нейронная сеть**  
Модель, состоящая из взаимосвязанных нейронов (узлов), имитирующих работу человеческого мозга для обработки информации.

**Feedforward Neural Network – Прямой нейронная сеть**  
Базовый тип сети, в которой данные проходят от входного слоя к выходному без обратных связей.

**Perceptron – Перцептрон**  
Элементарная модель нейрона, принимающая входные данные, обрабатывающая их и выдающая результат.

**Fully Connected Layer – Полносвязный слой**  
Слой, где каждый нейрон соединён с каждым нейроном предыдущего слоя.

**Activation Function – Функция активации**  
Функция, применяемая к сумме взвешенных входов нейрона для введения нелинейности (например, ReLU, sigmoid, tanh).

**Softmax – Софтмакс**  
Функция, преобразующая вектор значений в вероятностное распределение, часто используемая для многоклассовой классификации.

**Loss Function – Функция потерь**  
Мера ошибки, определяющая разницу между предсказанными и истинными значениями, которую модель стремится минимизировать.

**Gradient Descent – Метод градиентного спуска**  
Алгоритм оптимизации для корректировки весов модели в направлении наискорейшего уменьшения функции потерь.

**Momentum – Моментум**  
Метод ускорения сходимости оптимизации за счёт учёта предыдущих обновлений весов для сглаживания траектории.

**Adam – Адам**  
Популярный алгоритм оптимизации, объединяющий адаптивное вычисление моментов и моментум для ускорения сходимости.

**Backpropagation – Обратное распространение ошибки**  
Метод расчёта градиентов, используемый для обновления весов нейронной сети после каждого прохода обучения.

**Optimizer – Оптимизатор**  
Алгоритм (например, SGD, Adam, RMSprop), используемый для обновления параметров модели в процессе обучения.

**Regularization – Регуляризация**  
Набор техник для уменьшения переобучения модели, включая L1, L2, Dropout и Early Stopping.

**Dropout – Отбрасывание**  
Метод регуляризации, при котором случайно исключаются некоторые нейроны во время обучения для уменьшения зависимости от конкретных путей.

**Batch Normalization – Нормализация пакета**  
Техника, нормализующая входы каждого слоя по мини-пакетам данных для стабилизации и ускорения обучения.

**Learning Rate – Скорость обучения**  
Гиперпараметр, определяющий величину шага при обновлении весов модели.

**Learning Rate Scheduler – Планировщик скорости обучения**  
Механизм динамического изменения скорости обучения в процессе тренировки для улучшения сходимости.

**Epoch – Эпоха**  
Один полный проход по всему набору тренировочных данных.

**Mini-Batch – Мини-пакет**  
Небольшая часть обучающих данных, используемая для одного шага обновления весов, что делает обучение более стабильным.

**Weight Initialization – Инициализация весов**  
Процесс задания начальных значений параметров нейронной сети, влияющий на скорость и качество обучения.

**Transfer Learning – Трансферное обучение**  
Метод адаптации модели, предварительно обученной на одном наборе данных, для решения смежной задачи.

**Model Pruning – Обрезка модели**  
Процесс удаления избыточных параметров для уменьшения размера модели и повышения её эффективности.

**Quantization – Квантование**  
Снижение точности представления параметров модели для ускорения вычислений и экономии памяти.

**Ensemble Methods – Ансамблевые методы**  
Подход, при котором несколько моделей объединяются для получения более точных и устойчивых предсказаний.

**Cross-Validation – Кросс-валидация**  
Метод оценки обобщающих способностей модели путём разбиения данных на несколько обучающих и тестовых подмножеств.

**Residual Network (ResNet) – Остаточная сеть**  
Архитектура с пропускными (skip) связями, позволяющими строить очень глубокие сети и избегать проблемы затухающего градиента.

**Graph Neural Network (GNN) – Графовая нейронная сеть**  
Модель, предназначенная для обработки структурированных данных в виде графов, где связи между объектами имеют большое значение.

**Capsule Network – Капсульная сеть**  
Архитектура, стремящаяся сохранить пространственные иерархии между объектами, что полезно для задач распознавания изображений.

**DropConnect – Отбрасывание связей**  
Метод регуляризации, при котором случайным образом исключаются отдельные веса вместо нейронов целиком.

**Sparse Coding – Разреженное кодирование**  
Техника представления данных с использованием небольшого числа активных элементов, что способствует экономичному описанию информации.

---

## LLM — большие языковые модели

**LLM (Large Language Model) – Большая языковая модель**  
Комплексная нейросеть, обученная на огромном количестве текстовых данных для генерации, анализа и понимания естественного языка.

**Language Modeling – Языковое моделирование**  
Задача предсказания следующего слова или символа в последовательности, лежащая в основе обучения языковых моделей.

**Pretraining – Предобучение**  
Этап обучения модели на большом корпусе текстов без специализированной разметки, позволяющий освоить языковые закономерности.

**Masked Language Modeling – Замаскированное языковое моделирование**  
Метод, при котором часть токенов заменяется специальными масками, а модель учится предсказывать замаскированные элементы.

**Causal Language Modeling – Причинное языковое моделирование**  
Обучение модели предсказывать следующий токен на основе предыдущих, характерное для автогенеративных моделей.

**Fine-tuning – Тонкая настройка**  
Доработка предобученной модели на специализированном наборе данных для повышения эффективности в конкретной задаче.

**Parameter Efficient Fine-Tuning – Эффективное дообучение**  
Методы адаптации крупных моделей с минимальными изменениями параметров (например, LoRA).

**Token – Токен**  
Минимальная единица текста (слово, часть слова или символ), с которой работает модель.

**Prompt – Промт/Запрос/Подсказка**  
Входной текст, подаваемый модели для генерации ответа, от которого зависит качество результата.

**Context Window – Контекстное окно**  
Ограничение на количество токенов, которое модель может одновременно учитывать, влияющее на «память» модели.

**In-context Learning – Обучение в контексте**  
Способность модели адаптироваться к задаче на основе примеров, приведённых непосредственно в запросе.

**Zero-shot Learning – Обучение без примеров**  
Возможность выполнения задачи на основе описания без предварительного примера.

**Few-shot Learning – Обучение с несколькими примерами**  
Метод, при котором в запросе предоставляется несколько примеров для лучшего понимания задачи.

**Chain-of-thought Reasoning – Цепочка рассуждений**  
Техника пошагового описания процесса решения задачи, повышающая прозрачность и точность вывода.

**Decoding – Декодирование**  
Процесс генерации текста из скрытых представлений модели; выбор алгоритма (жадный поиск, beam search) влияет на итоговый результат.

**Temperature – Температура**  
Гиперпараметр, регулирующий степень случайности в процессе генерации текста: низкое значение обеспечивает детерминированность, высокое – разнообразие.

**Beam Search – Поиск по лучу**  
Алгоритм, одновременно рассматривающий несколько вариантов продолжения текста для выбора наиболее вероятного.

**Knowledge Distillation – Дистилляция знаний**  
Процесс переноса знаний от большой модели к меньшей для сокращения вычислительных затрат без значительной потери точности.

**Multitask Learning – Мультитаскинг**  
Обучение модели выполнению нескольких задач одновременно, что способствует улучшению обобщающих способностей.

**Model Scaling – Масштабирование модели**  
Увеличение числа параметров и вычислительной мощности модели для повышения её производительности.

**Attention Mask – Маска внимания**  
Механизм, определяющий, какие токены учитывать в процессе вычисления внимания, что помогает избегать влияния паддинга.

**Positional Encoding – Позиционное кодирование**  
Метод добавления информации о позиции токенов в последовательности для сохранения порядка слов в модели.

**Pretraining Objectives – Цели предобучения**  
Различные задачи (например, Next Sentence Prediction, Permuted Language Modeling) для обучения базовых языковых закономерностей.

**Scaling Laws – Законы масштабирования**  
Эмпирические зависимости, описывающие, как увеличение размера модели и объёма данных влияет на её производительность.

**Multilingual Modeling – Мультиязычное моделирование**  
Подход, позволяющий обучать модели, способные работать с несколькими языками одновременно.

---

## Computer Vision

**Image Classification – Классификация изображений**  
Задача определения категории, к которой принадлежит изображение.

**Object Detection – Обнаружение объектов**  
Процесс локализации и классификации объектов на изображении с помощью рамок или масок.

**Semantic Segmentation – Семантическая сегментация**  
Разбиение изображения на области, соответствующие различным классам, без выделения отдельных экземпляров.

**Instance Segmentation – Сегментация объектов**  
Метод, позволяющий выделять отдельные объекты и определять их класс в рамках одного изображения.

**Convolutional Neural Network (CNN) – Сверточная нейронная сеть**  
Архитектура, оптимизированная для обработки изображений посредством применения свёрток.

**Convolution – Свёртка**  
Операция, применяемая для извлечения локальных признаков из изображения с использованием фильтров.

**Pooling – Подвыборка**  
Операция уменьшения пространственных размеров представлений (например, max pooling, average pooling) для выделения устойчивых признаков.

**Feature Map – Карта признаков**  
Результат применения свёрточного фильтра к изображению, отражающий обнаруженные признаки.

**Image Preprocessing – Предобработка изображений**  
Методы подготовки изображений к анализу (нормализация, изменение размера, аугментация).

**Image Augmentation – Аугментация изображений**  
Методы искусственного увеличения объёма данных посредством применения преобразований (поворот, масштабирование, отражение).

**Edge Detection – Выделение контуров**  
Алгоритмы для обнаружения границ объектов, способствующие выделению структурных особенностей изображения.

**Optical Flow – Оптический поток**  
Метод оценки движения объектов в последовательности изображений или видеопотоке.

**Pose Estimation – Определение позы**  
Задача определения положения и ориентации человека или объекта на изображении.

**Generative Adversarial Networks (GAN) в CV**  
Применение GAN для генерации реалистичных изображений, улучшения качества или стилизации.

**Visual Transformers – Визуальные трансформеры**  
Адаптация архитектуры трансформеров для задач компьютерного зрения, позволяющая моделям работать с изображениями напрямую.

**Region Proposal Network (RPN) – Сеть предложений регионов**  
Компонент, генерирующий кандидаты регионов на изображении для последующего обнаружения объектов.

**Feature Pyramid Network (FPN) – Сеть пирамиды признаков**  
Архитектура, объединяющая признаки на различных масштабах для улучшения точности обнаружения объектов.

**Style Transfer – Перенос стиля**  
Метод, позволяющий переносить художественный стиль одного изображения на контент другого.

**Super-Resolution – Сверхразрешение**  
Техника повышения разрешения изображения с восстановлением утраченных деталей.

**Semantic Instance Segmentation – Семантическая сегментация объектов**  
Комбинированный подход, объединяющий семантическую сегментацию и выделение отдельных экземпляров объектов.

---

## Reinforcement Learning

**Reinforcement Learning – Обучение с подкреплением**  
Область, в которой агент обучается на основе наград и штрафов, получаемых от взаимодействия с окружающей средой.

**Q-Learning – Q-обучение**  
Метод, при котором агент изучает оптимальную политику, оценивая Q-функцию — ожидаемую награду для пары состояние-действие.

**Deep Q-Network (DQN) – Глубокая Q-сеть**  
Комбинация Q-обучения и нейронных сетей для работы с высокоразмерными пространствами состояний.

**Policy Gradient – Градиент политики**  
Метод оптимизации, обновляющий параметры политики напрямую для максимизации ожидаемой награды.

**Actor-Critic – Актор-критик**  
Подход, где «актор» принимает решения, а «критик» оценивает их качество для корректировки стратегии.

**Proximal Policy Optimization (PPO) – Проксимальное оптимальное обучение политики**  
Алгоритм, обеспечивающий стабильное обновление политики посредством ограничения изменений в каждом шаге.

**Monte Carlo Tree Search (MCTS) – Монте-Карло поиск по дереву**  
Метод планирования, использующий симуляцию возможных исходов для принятия оптимальных решений.

**Exploration vs. Exploitation – Исследование vs. использование**  
Баланс между поиском новых стратегий и применением уже известных для максимизации награды.

**SARSA – SARSA алгоритм**  
Метод обучения с подкреплением, в котором обновление производится на основе текущего действия и следующего шага, отражая последовательность состояний и действий.

**Double DQN – Двойная глубокая Q-сеть**  
Улучшенная версия DQN, снижающая переоценку Q-значений за счёт двойного оценивания.

**Prioritized Experience Replay – Приоритетный повтор опыта**  
Метод, позволяющий чаще использовать для обучения наиболее информативные переходы из опыта агента.

**Multi-Agent Reinforcement Learning – Мультиагентное обучение с подкреплением**  
Подход, в котором несколько агентов одновременно обучаются взаимодействовать в общей среде.

**Inverse Reinforcement Learning – Обратное обучение с подкреплением**  
Метод, позволяющий восстановить функцию награды на основе наблюдаемого поведения эксперта.

---

## Additional Topics and Methods

**Self-Supervised Learning – Самостоятельное обучение**  
Метод, при котором модель обучается на данных без явных меток, извлекая скрытые закономерности.

**Meta-Learning – Мета-обучение**  
Подход, позволяющий моделям быстро адаптироваться к новым задачам с минимальным количеством примеров.

**Automated Machine Learning (AutoML) – Автоматизированное машинное обучение**  
Процесс автоматизации выбора моделей, гиперпараметров и этапов предобработки для оптимизации обучения.

**Adversarial Attacks – Атаки с использованием противоречивых примеров**  
Методы создания специально сформированных входных данных для проверки устойчивости моделей к ошибкам.

**Robustness – Надёжность**  
Способность модели сохранять работоспособность при изменениях входных данных или внешних условиях.

**Interpretability – Интерпретируемость**  
Способность модели предоставлять понятные объяснения своих решений, что важно для проверки корректности.

**Explainability – Объяснимость**  
Процесс разработки инструментов для демонстрации причин, по которым модель приняла то или иное решение.

**Continual Learning – Непрерывное обучение**  
Методика, позволяющая модели адаптироваться к новым данным, не забывая ранее изученное.

**Few-Shot Meta-Learning – Мета-обучение с малым количеством примеров**  
Подход, позволяющий эффективно обучаться на ограниченном объёме данных с учётом предыдущего опыта.

**Data Imbalance Handling – Работа с несбалансированными данными**  
Набор техник для корректного обучения на выборках с неравномерным распределением классов.

**Causal Inference in DL – Причинный вывод в глубоких моделях**  
Методы выявления и анализа причинно-следственных связей в данных с использованием глубокого обучения.

**Neural Tangent Kernel (NTK) – Ядерный подход нейронных касательных**  
Теоретическая модель, позволяющая анализировать поведение бесконечно широких нейронных сетей и их обучение.
